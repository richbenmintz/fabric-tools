{"cells":[{"cell_type":"code","source":["lakehouses = [\n","    {'workspace':'inventory-analytics-data', 'lakehouse':'lh_bronze','schema':'availability'}\n","    ]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"0fed8eff-cfe1-4e1c-9a76-43a369f39d2d","normalized_state":"finished","queued_time":"2025-08-07T17:20:06.1439902Z","session_start_time":null,"execution_start_time":"2025-08-07T17:20:06.1451961Z","execution_finish_time":"2025-08-07T17:20:06.4618777Z","parent_msg_id":"1cf9d539-2b5b-490a-87c4-5e66d4557cde"},"text/plain":"StatementMeta(, 0fed8eff-cfe1-4e1c-9a76-43a369f39d2d, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":19,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"8940178c-d6a8-44d9-86fe-b63054cac981"},{"cell_type":"code","source":["from pyspark.sql.functions import map_concat, col, lit, collect_list, concat_ws, concat, expr, col, max, min, current_timestamp, datediff, desc, map_entries, map_from_entries, struct\n","\n","def vacuum_and_optimize_all_tables(workspace, lakehouse, schema, retention_period=None):\n","    \"\"\"\n","    A function to vacuum and optimize all delta tables in a specific lakehouse/schema\n","\n","    Parameters\n","    ----------\n","        workspace : string\n","            The name of the fabric workspace where the lakehouse exists\n","        lakehouse : string\n","            The name of the Lakehouse where the schema/table(s) are located\n","        schema : string\n","            The name of the Schema where tables require hygiene\n","        retention_period: int, in hours, default=None\n","            The number of hours you would like to retain 7 days is 168 Hours; if no value added the default retention will be used, Databricks default is 7 days\n","    \"\"\"\n"," \n","    for i in get_all_tables(workspace, lakehouse, schema).selectExpr('tableName').collect():\n","        if do_vacuum_and_optimize(workspace, lakehouse, schema, i[0], retention_period):\n","            vacuum_table(workspace, lakehouse, schema, i[0], retention_period)\n","            optimize_table(workspace, lakehouse, schema, i[0])\n","\n","def get_all_tables(workspace, lakehouse, schema):\n","    \"\"\"\n","    A function to return all delta tables in the lakehouse\n","\n","    Parameters\n","    ----------\n","        workspace : string\n","            The name of the fabric workspace where the lakehouse exists\n","        lakehouse : string\n","            The name of the Lakehouse where the schema/table(s) are located\n","        schema : string\n","            The name of the Schema where tables require hygiene\n","    Returns\n","    -------\n","        dataframe\n","    \"\"\"\n","    df = spark.sql(f\"show tables from `{workspace}`.{lakehouse}.{schema}\")\n","    return df\n","\n","def do_vacuum_and_optimize(workspace, lakehouse, schema, delta_table,\n","                           retention_period=None) -> bool:\n","    \"\"\"\n","    A function to Check to see if Table has enough version or is old enough to optimize and vacuum\n","\n","    Parameters\n","    ----------\n","        workspace : string\n","            The name of the fabric workspace where the lakehouse exists\n","        lakehouse : string\n","            The name of the Lakehouse where the schema/table(s) are located\n","        schema : string\n","            The name of the Schema where table require hygiene\n","        delta_table: string\n","            The name of the delta table that requires hygiene\n","        retention_period: int, in hours, default=None\n","            The number of hours you would like to retain 7 days is 168 Hours; if no value added the default retention will be used, Databricks default is 7 days\n","    Returns\n","    -------\n","        bool:\n","            whether to vacuum and optimize table\n","    \"\"\"\n","    retention = 168 if retention_period is None else retention_period\n","    ret_period = int(retention / 24)\n","    try:\n","        table_delta_info = spark.sql(f\"describe history `{workspace}`.{lakehouse}.{schema}.{delta_table}\").groupBy().agg(max('version').alias('version'), \n","            datediff(current_timestamp(), min('timestamp')).alias('table_age')).collect()[0]\n","        if table_delta_info['version'] >= ret_period and table_delta_info['table_age'] > ret_period:\n","            return True\n","        else:\n","            return False\n","    except Exception:\n","        return False\n","\n","def vacuum_table(workspace, lakehouse, schema, delta_table, retention_period=None):\n","    \"\"\"\n","    A function to vacuum a delta table\n","\n","    Parameters\n","    ----------\n","         workspace : string\n","            The name of the fabric workspace where the lakehouse exists\n","        lakehouse : string\n","            The name of the Lakehouse where the schema/table(s) are located\n","        schema : string\n","            The name of the Schema where table require hygiene\n","        delta_table: string\n","            The name of the delta table that requires hygiene\n","        retention_period: int, in hours default=None\n","            The number of hours you would like to retain 7 days is 168 Hours; if no value added the default retention will be used, Databricks default is 7 days\n","    Returns\n","    -------\n","        In the case of an Exception a JSON object describing the error is returned\n","            {\"error-message\": f\"error vacuuming table {delta_db}.{delta_table}- {Exception}\"}\n","    \"\"\"\n","    try:\n","        retention_string = f\"RETAIN {retention_period} HOURS\" if retention_period is not None else \"\"\n","\n","        spark.sql(f\"vacuum  `{workspace}`.{lakehouse}.{schema}.{delta_table} {retention_string}\")\n","    except Exception as e:\n","        return {\"error-message\": f\"error vacuuming table `{workspace}`.{lakehouse}.{schema}.{delta_table} {retention_string} - {e}\"}\n","\n","def optimize_table(workspace, lakehouse, schema, delta_table):\n","    \"\"\"\n","    A function to optimize a delta table\n","\n","    Parameters\n","    ----------\n","        spark : spark context\n","            spark context passed from the calling spark instance\n","        delta_db : string\n","            The name of the delta db where the table exists\n","        delta_table : string\n","            The name of the table to be optimized\n","        catalog: string, default=None\n","            The Name of the Unity Catalog if exists, Defaults to None\n","    Returns\n","    -------\n","        In the case of an Exception a JSON object describing the error is returned\n","            {\"error-message\": f\"error optimizing table {delta_db}.{delta_table}- {Exception}\"}\n","    \"\"\"\n","\n","    try:\n","        spark.sql(f\"OPTIMIZE `{workspace}`.{lakehouse}.{schema}.{delta_table} \")\n","     \n","    except Exception as e:\n","        return {\n","            \"error-message\": f\"error optimizing table  `{workspace}`.{lakehouse}.{schema}.{delta_table}\"}"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"0fed8eff-cfe1-4e1c-9a76-43a369f39d2d","normalized_state":"finished","queued_time":"2025-08-07T17:20:11.4525973Z","session_start_time":null,"execution_start_time":"2025-08-07T17:20:11.4539029Z","execution_finish_time":"2025-08-07T17:20:11.7691188Z","parent_msg_id":"38f734b6-b829-45c0-be32-26b939c68c65"},"text/plain":"StatementMeta(, 0fed8eff-cfe1-4e1c-9a76-43a369f39d2d, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":30,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2c04f04f-16bb-4ef3-bfca-813ef5d6d9d6"},{"cell_type":"code","source":["spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"0fed8eff-cfe1-4e1c-9a76-43a369f39d2d","normalized_state":"finished","queued_time":"2025-08-07T17:19:11.0828152Z","session_start_time":null,"execution_start_time":"2025-08-07T17:19:23.9781984Z","execution_finish_time":"2025-08-07T17:19:24.2954736Z","parent_msg_id":"537e6770-c164-4df1-a9f1-b25dd4b1fbef"},"text/plain":"StatementMeta(, 0fed8eff-cfe1-4e1c-9a76-43a369f39d2d, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5001b723-26be-4ee5-baad-c6470ac91218"},{"cell_type":"code","source":["for v_and_o_task in lakehouses:\n","    vacuum_and_optimize_all_tables(v_and_o_task.get('workspace'),v_and_o_task.get('lakehouse'),v_and_o_task.get('schema'), 168)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"0fed8eff-cfe1-4e1c-9a76-43a369f39d2d","normalized_state":"finished","queued_time":"2025-08-07T17:20:22.040047Z","session_start_time":null,"execution_start_time":"2025-08-07T17:20:22.0411187Z","execution_finish_time":"2025-08-07T17:20:51.5394196Z","parent_msg_id":"ac547768-8dd6-47f6-a5c4-df2a86bbe625"},"text/plain":"StatementMeta(, 0fed8eff-cfe1-4e1c-9a76-43a369f39d2d, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":31,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"c31aa83f-7892-4560-a99a-369eb088fff5"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"dc640051-43f4-4445-90a4-23f33f9d7e27"}],"default_lakehouse":"dc640051-43f4-4445-90a4-23f33f9d7e27","default_lakehouse_name":"lh_bronze","default_lakehouse_workspace_id":"80c10ed4-052b-482f-bf0a-1cdfcb85b965"}}},"nbformat":4,"nbformat_minor":5}